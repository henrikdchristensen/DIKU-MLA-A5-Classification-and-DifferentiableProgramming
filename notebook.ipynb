{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sleep well (35 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "X_train = pd.read_csv('data\\Sleep-EDF-15_U-Time/X_train.csv', header=None)\n",
    "y_train = pd.read_csv('data\\Sleep-EDF-15_U-Time/y_train.csv', header=None)\n",
    "X_test = pd.read_csv('data\\Sleep-EDF-15_U-Time/X_test.csv', header=None)\n",
    "y_test = pd.read_csv('data\\Sleep-EDF-15_U-Time/y_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data understanding and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate class frequencies for training data, normalized to [0,1]\n",
    "class_frequencies = y_train.value_counts(normalize=True).sort_index()\n",
    "\n",
    "# Print each class frequency for training data\n",
    "print(\"Class frequencies for training data:\")\n",
    "for label, freq in class_frequencies.items():\n",
    "    print(f\"Class {label}: {freq*100:.2f}%\")\n",
    "\n",
    "# Plotting\n",
    "labels = [f\"Class {i}\" for i in range(0, 5)]\n",
    "freqs = class_frequencies.values * 100\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.bar(labels, freqs, color=plt.cm.tab10.colors, alpha=0.5)\n",
    "plt.ylabel('Frequency (%)')\n",
    "plt.title('Class Frequencies for Training Data')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.savefig('output/class_frequencies.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, zero_one_loss\n",
    "\n",
    "def evaluate_classifier(y_true_train, y_pred_train, y_true_test, y_pred_test, classifier_name):\n",
    "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
    "    train_report = classification_report(y_true_train, y_pred_train)\n",
    "\n",
    "    test_accuracy = accuracy_score(y_true_test, y_pred_test)\n",
    "    test_report = classification_report(y_true_test, y_pred_test)\n",
    "\n",
    "    print(f\"\\nEvaluation Metrics for {classifier_name}:\\n\")\n",
    "\n",
    "    print(f\"Train Accuracy: {train_accuracy:.2f}\")\n",
    "    print(f\"Train Error: {zero_one_loss(y_true_train, y_pred_train):.2f}\")\n",
    "    print(f\"Train Classification Report:\\n{train_report}\")\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "    print(f\"Test Error: {zero_one_loss(y_true_test, y_pred_test):.2f}\")\n",
    "    print(f\"Test Classification Report:\\n{test_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-nominal Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(estimator=LogisticRegression(max_iter=10000, n_jobs=-1),\n",
    "                           param_grid=param_grid,\n",
    "                           n_jobs=-1,\n",
    "                           cv=5,\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Predict and evaluate using the best model\n",
    "train_pred = best_model.predict(X_train)\n",
    "test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_classifier(y_train, train_pred, y_test, test_pred, f\"Logistic Regression (Best Model)\")\n",
    "\n",
    "# Plotting cross-validation results\n",
    "mean_train_scores = grid_search.cv_results_['mean_train_score']\n",
    "mean_test_scores = grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "# Bar chart settings\n",
    "index = np.arange(len(param_grid['C']))\n",
    "bar_width = 0.25\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.bar(index, 100-mean_train_scores*100, bar_width, label='Training Error', color='tab:blue', alpha=0.5)\n",
    "plt.bar(index + bar_width, 100-mean_test_scores*100, bar_width, label='Test Error', color='tab:red', alpha=0.5)\n",
    "\n",
    "# Set the x-tick positions and labels\n",
    "tick_positions = index + bar_width/2\n",
    "plt.xticks(tick_positions, param_grid['C'])\n",
    "\n",
    "plt.xlabel('C (Inverse Regularization Strength)')\n",
    "plt.ylabel('Error (%)')\n",
    "plt.ylim(0, 22)\n",
    "plt.title('Logistic Regression\\nMean Error vs. C (Inverse Regularization Strength)')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Grid search with return_train_score set to True\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(n_jobs=-1), \n",
    "                           param_grid=param_grid, \n",
    "                           n_jobs=-1, \n",
    "                           cv=5,\n",
    "                           verbose=3,\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Get the best estimator (model)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\")\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Predict and evaluate using the best model\n",
    "train_pred = best_model.predict(X_train)\n",
    "test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_classifier(y_train, train_pred, y_test, test_pred, f\"Random Forest (Best Model)\")\n",
    "\n",
    "# Plotting cross-validation results\n",
    "mean_train_scores = grid_search.cv_results_['mean_train_score']\n",
    "mean_test_scores = grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "# Bar chart settings\n",
    "index = np.arange(len(param_grid['n_estimators']))\n",
    "bar_width = 0.25\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.bar(index, 100-mean_train_scores*100, bar_width, label='Training Error', color='tab:blue', alpha=0.5)\n",
    "plt.bar(index + bar_width, 100-mean_test_scores*100, bar_width, label='Test Error', color='tab:red', alpha=0.5)\n",
    "\n",
    "# Set the x-tick positions and labels\n",
    "tick_positions = index + bar_width/2\n",
    "plt.xticks(tick_positions, param_grid['n_estimators'])\n",
    "\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Error (%)')\n",
    "plt.ylim(0, 22)\n",
    "plt.title('Random Forest\\nMean Error vs. Number of Trees')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest-Neighbor Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "# Rule of thumb: sqrt(n) where n is the number of samples: https://quantdev.ssri.psu.edu/sites/qdev/files/kNN_tutorial.html\n",
    "# 4 set in the training dataset and 1 set for validation. Total samples: 33724; validation samples: 33724/5 = 6745; training samples: 33724 - 6745 = 26979\n",
    "# K-neighbors should be an odd value in KNN to avoid ties at decision boundaries\n",
    "# where majority voting occurs. Using odd numbers helps prevent ties.\n",
    "param_grid = {'n_neighbors': range(1, 130, 2)}  # Considering odd values from 1 to 129 neighbors\n",
    "\n",
    "# Grid Search to find the best number of neighbors\n",
    "grid_search = GridSearchCV(estimator=KNeighborsClassifier(n_jobs=-1),\n",
    "                           param_grid=param_grid,\n",
    "                           return_train_score=True,\n",
    "                           n_jobs=-1,\n",
    "                           cv=5)\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Train the model with the best number of neighbors\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict and evaluate using the best model\n",
    "train_pred = best_model.predict(X_train)\n",
    "test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_classifier(y_train, train_pred, y_test, test_pred, \"K-Nearest Neighbors\")\n",
    "\n",
    "# Plot the cross-validation results\n",
    "k_values = np.array(grid_search.cv_results_['param_n_neighbors'].data, dtype=float)\n",
    "train_error = np.array(100 - grid_search.cv_results_['mean_train_score']*100, dtype=float)\n",
    "test_error = np.array(100 - grid_search.cv_results_['mean_test_score']*100, dtype=float)\n",
    "train_std = np.array(grid_search.cv_results_['std_train_score']*100, dtype=float)\n",
    "test_std = np.array(grid_search.cv_results_['std_test_score']*100, dtype=float)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(k_values, train_error, label=\"Train error\", color=\"tab:blue\", alpha=0.6)\n",
    "plt.fill_between(k_values, train_error - train_std, train_error + train_std, color=\"tab:blue\", alpha=0.1)  # Variance for train error\n",
    "plt.plot(k_values, test_error, label='Test error', color=\"tab:red\", alpha=0.6)\n",
    "plt.fill_between(k_values, test_error - test_std, test_error + test_std, color=\"tab:red\", alpha=0.1)  # Variance for test error\n",
    "\n",
    "plt.title('KNN\\nMean Train- and Test Error vs. Number of Neighbors')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Error (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/knn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Invariance and normalization (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Original data\n",
    "data = {\n",
    "    \"Mr. Good\":     np.array([47, 35]),\n",
    "    \"Mr. Bad\":      np.array([22, 40]),\n",
    "    \"Mr. Unknown\":  np.array([21, 36])\n",
    "}\n",
    "\n",
    "# Calculate means\n",
    "mean_x = np.mean([data[\"Mr. Good\"][0], data[\"Mr. Bad\"][0]])\n",
    "mean_y = np.mean([data[\"Mr. Good\"][1], data[\"Mr. Bad\"][1]])\n",
    "\n",
    "print(f\"Mean of age: {mean_x}\")\n",
    "print(f\"Mean of income: {mean_y}\\n\")\n",
    "\n",
    "# Center the data (zero mean)\n",
    "centered_data = {key: value - [mean_x, mean_y] for key, value in data.items()}\n",
    "\n",
    "for key, value in centered_data.items():\n",
    "    print(f\"Centered coordinates for {key}: {value}\")\n",
    "\n",
    "# Calculate variances\n",
    "var_x = np.var([centered_data[\"Mr. Good\"][0], centered_data[\"Mr. Bad\"][0]])\n",
    "var_y = np.var([centered_data[\"Mr. Good\"][1], centered_data[\"Mr. Bad\"][1]])\n",
    "\n",
    "print(f\"\\nVariance of age: {var_x}\")\n",
    "print(f\"Variance of income: {var_y}\")\n",
    "\n",
    "# Calculate standard deviations\n",
    "std_x = np.sqrt(var_x)\n",
    "std_y = np.sqrt(var_y)\n",
    "\n",
    "print(f\"\\nStandard deviation of age: {std_x}\")\n",
    "print(f\"Standard deviation of income: {std_y}\\n\")\n",
    "\n",
    "# Normalize the data (variance one)\n",
    "normalized_data = {key: [value[0]/std_x, value[1]/std_y] for key, value in centered_data.items()}\n",
    "\n",
    "for key, value in normalized_data.items():\n",
    "    print(f\"Normalized coordinates for {key}: {value}\")\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "# Original data plot\n",
    "for key, value in data.items():\n",
    "    ax[0].scatter(value[0], value[1], label=key, alpha=0.6)\n",
    "ax[0].set_title(\"Original data\")\n",
    "ax[0].set_xlabel(\"Age (years)\")\n",
    "ax[0].set_ylabel(\"Income (thousands of $)\")\n",
    "ax[0].grid(True)\n",
    "ax[0].set_xlim(0, 50)\n",
    "ax[0].set_ylim(0, 50)\n",
    "\n",
    "# Normalized data plot\n",
    "for key, value in normalized_data.items():\n",
    "    ax[1].scatter(value[0], value[1], label=key, alpha=0.6)\n",
    "ax[1].set_title(\"Normalized data\")\n",
    "ax[1].set_xlabel(\"Age (normalized)\")\n",
    "ax[1].set_ylabel(\"Income (normalized)\")\n",
    "ax[1].grid(True)\n",
    "\n",
    "# Add a single legend for the entire figure\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.04), ncol=3)\n",
    "\n",
    "plt.title(\"Example of Invariance and Normalization\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/invariance_and_normalization.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Differentiable programming (35 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Parameter affecting the shape of the quadratic function\n",
    "p_f = 0.5  \n",
    "\n",
    "# Quadratic function definition\n",
    "def f(x, y):\n",
    "    \"\"\"Compute the value of the function for given x and y.\"\"\"\n",
    "    return (p_f*x)**2 + y**2 + p_f*x*y\n",
    "\n",
    "def gradient_descent_plot(optimizer_type, p_x, p_y, r, n_iter):\n",
    "    \"\"\"\n",
    "    Plot the contour of the function and the gradient descent path.\n",
    "    \n",
    "    Parameters:\n",
    "    - p_x, p_y: Lists containing x and y coordinates of the gradient descent path.\n",
    "    - r: Range for plotting.\n",
    "    - n_iter: Number of iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "\n",
    "    # Generate meshgrid for contour plot\n",
    "    x = np.linspace(-r, r, 50)\n",
    "    y = np.linspace(-r, r, 50)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    # Draw contour lines and fill\n",
    "    contours = plt.contour(X, Y, Z, [0.01, 0.05, 0.1, 0.5, 1.], colors='grey')\n",
    "    plt.clabel(contours, inline=True, fontsize=6)\n",
    "    plt.imshow(Z, extent=[-r, r, -r, r], origin='lower', cmap='RdGy', alpha=0.5)\n",
    "\n",
    "    # Mark the optimum point\n",
    "    plt.plot(0, 0, 'x', c='k')\n",
    "\n",
    "    # Plot arrows showing the gradient descent path\n",
    "    for i in range(n_iter):\n",
    "        plt.arrow(p_x[i], p_y[i], p_x[i+1]-p_x[i], p_y[i+1]-p_y[i], width=.005, head_width=.045, head_length=.025, length_includes_head=True, fc='b', ec='b', zorder=10)\n",
    "    \n",
    "    # Save plot\n",
    "    plt.title(f'Gradient Descent with {optimizer_type}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'output/gradient_descent_{optimizer_type}.png')\n",
    "    plt.show()\n",
    "\n",
    "def perform_gradient_descent(optimizer_type, eta, n_iter, r):\n",
    "    \"\"\"\n",
    "    Perform gradient descent optimization and store the path.\n",
    "    \n",
    "    Parameters:\n",
    "    - optimizer_type: Type of optimizer to use (\"SGD\" or \"Adam\").\n",
    "    - eta: Learning rate.\n",
    "    - n_iter: Number of iterations.\n",
    "    - r: Range for initial point.\n",
    "    \"\"\"\n",
    "    # Initialize starting point\n",
    "    x = torch.ones(1, requires_grad=True)\n",
    "    y = torch.ones(1, requires_grad=True)\n",
    "    with torch.no_grad():\n",
    "        x *= 0.9*r\n",
    "        y *= 0.8*r\n",
    "\n",
    "    # Lists to store path of gradient descent\n",
    "    p_x = [x.item()]\n",
    "    p_y = [y.item()]\n",
    "\n",
    "    # Choose optimizer based on input\n",
    "    if optimizer_type == \"SGD\":\n",
    "        optimizer = torch.optim.SGD([x, y], lr=eta)\n",
    "    elif optimizer_type == \"Adam\":\n",
    "        optimizer = torch.optim.Adam([x, y], lr=eta)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer type\")\n",
    "\n",
    "    # Gradient descent loop\n",
    "    for i in range(n_iter):\n",
    "        optimizer.zero_grad()\n",
    "        f(x, y).backward()\n",
    "        optimizer.step()\n",
    "        p_x.append(x.item())\n",
    "        p_y.append(y.item())\n",
    "\n",
    "    # Plot the results of gradient descent\n",
    "    gradient_descent_plot(optimizer_type, p_x, p_y, r, n_iter)\n",
    "\n",
    "# Range for plotting the function\n",
    "r = 1.\n",
    "\n",
    "# Perform and plot gradient descent using SGD optimizer\n",
    "perform_gradient_descent(\"SGD\", eta = 0.5, n_iter = 15, r = r)\n",
    "\n",
    "# Perform and plot gradient descent using Adam optimizer\n",
    "perform_gradient_descent(\"Adam\", eta = 0.5, n_iter = 15, r = r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
